{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beigebook\n",
    "\n",
    "Commonly known as the Beige Book, this report is published eight times per year. Each Federal Reserve Bank gathers anecdotal information on current economic conditions in its District through reports from Bank and Branch directors and interviews with key business contacts, economists, market experts, and other sources. The Beige Book summarizes this information by District and sector. An overall summary of the twelve district reports is prepared by a designated Federal Reserve Bank on a rotating basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4.dammit import EncodingDetector\n",
    "import requests\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous statements\n",
    "previous_beigebooks = pd.read_csv('bbooks.csv')\n",
    "\n",
    "# Create list of dates for beige books that have already been scraped\n",
    "saved_dates = previous_beigebooks['date'].drop_duplicates().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This page has the links of all the beige books for the current year \n",
    "base_page = \"https://federalreserve.gov/monetarypolicy/beige-book-default.htm\"\n",
    "\n",
    "# Download html\n",
    "base_html = requests.get(base_page)\n",
    "\n",
    "# Parse HTML to bs4 object\n",
    "base_soup = BeautifulSoup(base_html.text, \"html.parser\")\n",
    "\n",
    "# For safety, find the year on the webpage and save for later\n",
    "link_year = base_soup.find(id='year').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store beige book links in\n",
    "current_year_links = []\n",
    "\n",
    "# Find the table that contains the dates and links\n",
    "current_link_table = base_soup.find('tbody')\n",
    "\n",
    "# Parse the table to find the links for the individual beige books and dates\n",
    "beigebook_dates = current_link_table.find_all('tr')\n",
    "\n",
    "\n",
    "for date_section in beigebook_dates:\n",
    "    link_date = date_section.find('td').text+', '+link_year\n",
    "    formatted_date = dt.datetime.strptime(str(link_date), '%B %d, %Y').strftime('%Y%m%d')\n",
    "    \n",
    "# If we already have the beigebook parsed and saved, skip\n",
    "    \n",
    "    if formatted_date not in str(saved_dates):\n",
    "        try:\n",
    "            date_html_link = date_section.find('a', text='HTML')\n",
    "            link_append='https://www.federalreserve.gov'+date_html_link.attrs['href']\n",
    "            current_year_links.append([formatted_date, link_append])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "# Create DataFrame of dates and links\n",
    "book_link_table = pd.DataFrame(current_year_links, columns = ['date','link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sections(date, link):\n",
    "    \"\"\"Given a date and link, download the html text and parse in to individual regions and sections\"\"\"\n",
    "\n",
    "    text_html = requests.get(link)\n",
    "    soupy_book = BeautifulSoup(text_html.text, \"html.parser\")\n",
    "    paragraphs = soupy_book.find_all('p')\n",
    "    \n",
    "    sections = []\n",
    "\n",
    "    for p in paragraphs:\n",
    "        text_break = p.find('br')\n",
    "        if text_break != None:\n",
    "            section = text_break.previous\n",
    "            text = text_break.next.strip('\\n')\n",
    "            \n",
    "            # There is no previous h4 element, assume it is the National Summary\n",
    "            try:\n",
    "                region = text_break.find_previous('h4').text\n",
    "            except:\n",
    "                region = 'National Summary'\n",
    "            \n",
    "            data = [date,section,text,region]\n",
    "            sections.extend([data]) \n",
    "\n",
    "    text_df = pd.DataFrame(sections, columns = ['date','section','text','region'])\n",
    "\n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "# If there are reports available that we haven't already downloaded, parse and save those reports\n",
    "if len(book_link_table) > 0:\n",
    "    print('running')\n",
    "    \n",
    "    # create DataFrame for new beige books\n",
    "    add_books = pd.DataFrame()\n",
    "    \n",
    "    for row in book_link_table.index:\n",
    "        book = fetch_sections(book_link_table.loc[row,'date'], book_link_table.loc[row,'link'])\n",
    "\n",
    "        add_books = add_books.append(book, sort=False)\n",
    "\n",
    "    all_books = previous_beigebooks.append(add_books, sort=False)\n",
    "    all_books['date'] = all_books['date'].apply(lambda x: dt.datetime.strptime(str(x), '%Y%m%d').strftime('%Y%m%d'))\n",
    "    all_books = all_books.sort_values('date', ascending=False)\n",
    "\n",
    "    all_books.to_csv('bbooks.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    all_books = previous_beigebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOMC_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.dammit import EncodingDetector\n",
    "import requests\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomc_statement_links = []\n",
    "\n",
    "base_url = 'https://www.federalreserve.gov/newsevents/pressreleases/'\n",
    "\n",
    "year_string = str(dt.date.today().year)\n",
    "\n",
    "resp = requests.get(base_url+year_string+'-press.htm')\n",
    "\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "statements = soup.find_all('a', href=re.compile('^/newsevents/pressreleases/monetary\\d{8}a.htm'))\n",
    "\n",
    "statement_links = [statement.attrs['href'] for statement in statements]\n",
    "\n",
    "for link in statement_links:\n",
    "    fomc_statement_links.append(str(\"https://www.federalreserve.gov\"+link))\n",
    "\n",
    "def _date_from_link(link):\n",
    "    date = re.findall('[0-9]{8}', link)[0]\n",
    "    date = \"{}/{}/{}\".format(date[4:6], date[6:], date[:4])\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_df = pd.DataFrame()\n",
    "\n",
    "for link in fomc_statement_links:\n",
    "\n",
    "    page = requests.get(link)\n",
    "    soup2 = BeautifulSoup(page.text, \"html.parser\")\n",
    "    try:\n",
    "        tags2 = soup2.find(id='article')\n",
    "        paragraphs = tags2.findAll('p')\n",
    "    except:\n",
    "        paragraphs = soup2.findAll('p')\n",
    "\n",
    "    statement = \"\\n\\n\".join([text.get_text().strip() for text in paragraphs])\n",
    "    date= _date_from_link(link)\n",
    "\n",
    "    statements_df = statements_df.append(pd.DataFrame([[date, statement]],columns=['date','statement']))\n",
    "\n",
    "statements_df = statements_df.set_index('date')\n",
    "statement_dates = statements_df.index.tolist()\n",
    "\n",
    "previous_statements = pd.read_csv('fomcstatements.csv', index_col='date')\n",
    "\n",
    "keep_dates = statements_df.loc[[word for word in statement_dates if word not in previous_statements.index.tolist()]]\n",
    "\n",
    "if len(keep_dates) > 0:\n",
    "    print('Adding statements...')\n",
    "    fed_statements = previous_statements.append(keep_dates)\n",
    "\n",
    "    fed_statements.to_csv('fomcstatements.csv')\n",
    "    \n",
    "    else:\n",
    "        fed_statements = previous_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_statements.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
